---
title: " The effect of Leaky ReLUs on the training and generalization of overparameterized
  networks "
software: " https://github.com/sli743/leakyReLU "
abstract: " We investigate the training and generalization errors of overparameterized
  neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions.
  More specifically, we carefully upper bound both the convergence rate of the training
  error and the generalization error of such NNs and investigate the dependence of
  these bounds on the Leaky ReLU parameter, $\\alpha$. We show that $\\alpha =-1$,
  which corresponds to the absolute value activation function, is optimal for the
  training error bound. Furthermore, in special settings, it is also optimal for the
  generalization error bound. Numerical experiments empirically support the practical
  choices guided by the theory. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: guo24c
month: 0
tex_title: " The effect of Leaky {ReLUs} on the training and generalization of overparameterized
  networks "
firstpage: 4393
lastpage: 4401
page: 4393-4401
order: 4393
cycles: false
bibtex_author: Guo, Yinglong and Li, Shaohan and Lerman, Gilad
author:
- given: Yinglong
  family: Guo
- given: Shaohan
  family: Li
- given: Gilad
  family: Lerman
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/guo24c/guo24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
