---
title: " On learning history-based policies for controlling Markov decision processes "
software: " https://github.com/gp1702/history-based-policies "
abstract: " Reinforcement learning (RL) folklore suggests that methods of function
  approximation based on history, such as recurrent neural networks or state abstractions
  that include past information, outperform those without memory, because function
  approximation in Markov decision processes (MDP) can lead to a scenario akin to
  dealing with a partially observable MDP (POMDP). However, formal analysis of history-based
  algorithms has been limited, with most existing frameworks concentrating on features
  without historical context. In this paper, we introduce a theoretical framework
  to examine the behaviour of RL algorithms that control an MDP using feature abstraction
  mappings based on historical data. Additionally, we leverage this framework to develop
  a practical RL algorithm and assess its performance across various continuous control
  tasks. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: patil24b
month: 0
tex_title: " On learning history-based policies for controlling {M}arkov decision
  processes "
firstpage: 3511
lastpage: 3519
page: 3511-3519
order: 3511
cycles: false
bibtex_author: Patil, Gandharv and Mahajan, Aditya and Precup, Doina
author:
- given: Gandharv
  family: Patil
- given: Aditya
  family: Mahajan
- given: Doina
  family: Precup
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/patil24b/patil24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
