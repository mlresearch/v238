---
title: " On the Model-Misspecification in Reinforcement Learning "
abstract: " The success of reinforcement learning (RL) crucially depends on effective
  function approximation when dealing with complex ground-truth models. Existing sample-efficient
  RL algorithms primarily employ three approaches to function approximation: policy-based,
  value-based, and model-based methods. However, in the face of model misspecification—a
  disparity between the ground-truth and optimal function approximators—it is shown
  that policy-based approaches can be robust even when the policy function approximation
  is under a large \\emph{locally-bounded} misspecification error, with which the
  function class may exhibit a $\\Omega(1)$ approximation error in specific states
  and actions, but remains small on average within a policy-induced state distribution.
  Yet it remains an open question whether similar robustness can be achieved with
  value-based and model-based approaches, especially with general function approximation.
  To bridge this gap, in this paper we present a unified theoretical framework for
  addressing model misspecification in RL. We demonstrate that, through meticulous
  algorithm design and sophisticated analysis, value-based and model-based methods
  employing general function approximation can achieve robustness under local misspecification
  error bounds. In particular, they can attain a regret bound of $\\widetilde{O}\\left(\\mathrm{poly}(dH)\\cdot(\\sqrt{K}
  + K\\cdot\\zeta) \\right)$, where $d$ represents the complexity of the function
  class, $H$ is the episode length, $K$ is the total number of episodes, and $\\zeta$
  denotes the local bound for misspecification error. Furthermore, we propose an algorithmic
  framework that can achieve the same order of regret bound without prior knowledge
  of $\\zeta$, thereby enhancing its practical applicability. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24m
month: 0
tex_title: " On the Model-Misspecification in Reinforcement Learning "
firstpage: 2764
lastpage: 2772
page: 2764-2772
order: 2764
cycles: false
bibtex_author: Li, Yunfan and Yang, Lin
author:
- given: Yunfan
  family: Li
- given: Lin
  family: Yang
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/li24m/li24m.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
